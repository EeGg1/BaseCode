#temp
MMD:
  KERNEL: rbf
  GROUP_SIZE: 16
  STRIDE: 16
# number of gpus per node. per node -> 서버 여러개 쓸 때 
NUM_GPUS: 4
VISIBLE_DEVICES: 0,1,2,3
# directory to save result txt file
RESULT_DIR: ./results/
DATA_LOADER:
  NUM_WORKERS: 4
  PREFETCH_FACTOR: 2
  PIN_MEMORY: True
  PERSISTENT_WORKERS: True
# random seed number
SEED: 0 #! 체크
DATA:
  BASE_DIR: ./data
  NAME: weather
  SEQ_LEN: &seq_len_anchor 96 # encoder에 들어가는 window 길이 (look back window 보통 고정)
  LABEL_LEN: &label_len_anchor 48 # decoder embedding으로 들어가는 길이 (seq_len 의 절반이네 여기선) (itransformer에서는 필요없음)
  PRED_LEN: &pred_len_anchor 96 # prediction 길이 (이걸 변화시킨다 주로)
  SCALE: standard # 초기 전처리 normalization 방법 # standard, min-max
  FEATURES: M # prediction target이 multivariate이고 'S'는 input은 multivariate 예측 univariate
  TARGET_START_IDX: 0 # column 날린기준, prediction target이 시작하는 column index (예측하는 변수가 뒤에 몰려있어야 함)
  TIMEENC: 0 # data에 time stamp가 어떻게 되었냐에 따라 0, 1
  FREQ: h # h or t. temporal embedding. t는 minute embedding 포함. h는 hour, weekday, day, month embedding (iTransformer에서는 필요없음)
TRAIN:
  ENABLE: True # main.py에서 training 할건지 여부
  SPLIT: train
  BATCH_SIZE: &batch_size_anchor 256 # should be same as train_micro_batch_size_per_gpu in deepspeed_config.json while using deepspeed
  SHUFFLE: True
  DROP_LAST: True # 전체 dataset 길이가 batch_size로 나누어 떨어지지 않을 때 마지막 batch를 버릴지 여부
  CHECKPOINT_DIR: ./checkpoints/ # directory to save checkpoints
  RESUME: '' # path to checkpoint to resume training
  CHECKPOINT_PERIOD: 10 # epoch period to save checkpoints
  EVAL_PERIOD: 1 # epoch period to evaluate on a validation set
  PRINT_FREQ: 1 # iteration frequency to print progress meter
  BEST_METRIC_INITIAL: inf # MSE 나 MAE로 재는데 best model tracking 하기 위한거라서 초기값은 무한대로
  BEST_LOWER: True # best metric이 낮을수록 좋은지 높을수록 좋은지
  USE_DEEPSPEED: True # deepspeed 사용 여부, use bash script/deepspeed.sh instead of python main.py
  DEEPSPEED_CONFIG: ./config/deepspeed_config.json # deepspeed config file path
  TYPE: "fp32" # fp32, fp16, bf16
VAL:
  SPLIT: val
  BATCH_SIZE: 256
  SHUFFLE: False
  DROP_LAST: False
  VIS: False
TEST:
  ENABLE: True # main.py에서 test 할건지 여부
  SPLIT: test
  BATCH_SIZE: 1
  SHUFFLE: False
  DROP_LAST: False
  CHECKPOINT_DIR: ./checkpoints/ # directory to load trained checkpoints
  VIS_ERROR: False # Error 보여줄건지 
  VIS_DATA: True # TOP, WORST data 보여줄건지 
  VIS_DATA_NUM: 1 # TOP, WORST 몇개 보여줄건지
  PREDICTION_ERROR_DIR: "" # 불러와서 쓰고 싶으면, 평상시에는 ""로 놓고
  PREDICTION_ERROR_TYPE: MAE # MAE, MSE
  APPLY_MOVING_AVERAGE: False # True로 하면 moving average 적용, 시간에 따른 변화를 보기 위함 smoothing (TTA 관련 체크를 위해)
  # MOVING_AVERAGE_WINDOW: 100 
MODEL_NAME: transformer_mlp
MODEL:
  LOSS_USE_MSE: false #MUST ERASE!! JIWON
  task_name: long_term_forecast
  seq_len: *seq_len_anchor
  label_len: *label_len_anchor
  pred_len: *pred_len_anchor
  normalization: revin # non-stationary, revin, san, dish-ts
  e_layers: 3
  # d_layers: 1 
  factor: 3 # Prob Attention(probabilistic attention)에서 쓰는데 informer에서 씀
  # num_kernels: 6 # for Inception

  d_model: 512 # embedding dimension
  # values: [256, 512]    
  d_ff: 1024 # feed forward dimension
  pos_emb: 8 # d_model // pos_emb
  var_emb: 4 # d_model // var_emb
  # values: [4, 8]

  # d_state: 64
  # values: [64, 128]   
  # d_conv: 8 #4
  # values: [4, 8]   
  # expand: 2
  moving_avg: 25 # window size of moving average 라는데 autoformer에서 쓰는거 같다
  output_attention: False # whether the attention weights are returned by the forward method of the attention class
  dropout: 0.1 # 보통 0.1 많이 쓰는 것 같다
  n_heads: 8
  # values: [4, 8]
  activation: gelu
  METRIC_NAMES:
    - MAE
  LOSS_NAMES:
    - MSE_MMD_Loss
  embed: timeF
  freq: h
SOLVER:
  START_EPOCH: 0
  MAX_EPOCH: 200
  OPTIMIZING_METHOD: adamW # optimizer.py
  # values: ['adamW', 'adam']

  SAM_rho: 0.05
  BASE_LR: 0.0001 # warmup end learning rate
  #    distribution: q_log_uniform_values
  #    max: 0.003
  #    min: 0.000001
  #    q:   0.000001
  # sam 안쓸땐 1e-4 ~ 1e-7 쯤이 좋은듯
  # sam 쓸땐 1e-3 ~ 1e-6 쯤이 좋은듯

  WEIGHT_DECAY: 0.003
  # distribution: q_log_uniform_values
  # max: 0.02
  # min: 0.000001
  # q:   0.000001 
  # 그렇게 크게 성능에 영향을 주지 않음

  LR_POLICY: cosine # cosine / decay / 비워두면 base_lr로 돌아감
  # values: ['cosine', 'decay']

  COSINE_END_LR: 0.0000001 # 더이상 감소 안하고 여기서 멈춤
  COSINE_AFTER_WARMUP: True # warmup 없는 cosine만 하려면 false, warmup 있는 cosine 하려면 true
  WARMUP_EPOCHS: 1 # linear warmup epoch
  # distribution: q_uniform
  # max: 3.0
  # min: 1.0
  # q:   1.0
  WARMUP_START_LR: 0.0 # warmup start learning rate 
  # LR_DECAY_STEP: 1   # decay policy인 경우
  # LR_DECAY_RATE: #1.0 # decay policy인 경우
  #     distribution: q_uniform
  #     max: 1
  #     min: 0.85
  #     q:   0.01
# learning rate of last fc layer is scaled by fc_lr_ratio
# FC_LR_RATIO: 10.0

WANDB:
  ENABLE: True # wandb on/off # sweep 키면 자동으로 True 됨 #! 체크
  PROJECT: BaseCode
  NAME: transformer_mlp_weather_200_epochs_MMD_rbf_gs16_st16_usemse_bs256
  JOB_TYPE: train # train or eval
  NOTES: '' # a description of this run
  DIR: ./
  VIS_TRAIN_SCORE: true
  VIS_TEST_SCORE: true
  VIS_TEST_DATA: true # raw data 찍기
  VIS_TRAIN_TEST_HISTOGRAM: true
SWEEP: # 추천 방법: random search 해서 좋은 지점 대략 파악 -> grid search. bayes 잘 안됨.
  ENABLE: False #! 체크
  RESUME: False # resume from previous sweep
  # SWEEP_ID: # resume인 경우 필요

  COUNT: None # None 주면 무한대로 돌아감. manually stop 해야함.
  method: bayes # random, grid, bayes
  metric:
    name: Val/MAE #Val/Loss MSE #Test/test_mse #  # metric to optimize
    goal: minimize # minimize or maximize
  early_terminate:
    type: hyperband
    min_iter: 20 # 최소 metric이 찍힌 횟수. 
    eta: 2
# sweep 하고 싶은 파라미터만 변경. 예시 아래. categorical, distribution 예시.
# values: [1,2,3]
# OR
# distribution: https://docs.wandb.ai/guides/sweeps/sweep-config-keys#distribution-options-for-random-and-bayesian-search
# max: 0.001
# min: 0.00001
# q:   0.00002 (선택되는 최소 단위. 이거 설정 안하면 0.1285487203 이렇게 선택될수도.)
